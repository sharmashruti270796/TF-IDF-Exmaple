{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import operator\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy\n",
    "from operator import itemgetter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(corpus):\n",
    "    sent_count=0\n",
    "    main=[] \n",
    "    if isinstance(corpus,(list)):\n",
    "        for sent in corpus:\n",
    "            sen_list=[]\n",
    "            for word in sent.split():\n",
    "                sen_list.append(word)\n",
    "            main.append(sen_list)\n",
    "        unique=set()\n",
    "        tf=[]\n",
    "        for each_sent in main:\n",
    "            n=len(each_sent)\n",
    "            for each_word in each_sent:\n",
    "                unique.add(each_word)\n",
    "                word_freq=dict(Counter(each_word.split()))\n",
    "                for k,v in word_freq.items():\n",
    "                    term_freq=float(v/n) \n",
    "                    tf.append(term_freq)\n",
    "            vocab={j:i for i,j in enumerate(sorted(unique))}\n",
    "                \n",
    "                            \n",
    "                    \n",
    "        store=[]\n",
    "        list_idf=[]\n",
    "        feature_names=sorted(unique)\n",
    "        for each_word in sorted(unique):  \n",
    "            count=0 \n",
    "            for i in range(len(main)):\n",
    "                 if each_word in main[i]:\n",
    "                    count+=1    \n",
    "            store.append(count)  \n",
    "            for i in range(0,len(store)):  \n",
    "                idf1=1+(math.log((1+len(main))/(1+store[i])))   \n",
    "            list_idf.append(idf1)\n",
    "        idf=dict(zip(vocab,list_idf))  \n",
    "        \n",
    "          \n",
    "                       \n",
    "    return  idf,vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(idf,vocab):\n",
    "    rows=[]\n",
    "    cols=[]\n",
    "    values=[]\n",
    "    if isinstance(corpus,(list)):\n",
    "        for index,sentence in enumerate(tqdm(corpus)):\n",
    "            word_freq=dict(Counter(sentence.split()))\n",
    "            for word,frequency in word_freq.items():\n",
    "                term_freq=frequency/len(word_freq)\n",
    "                tf={word:term_freq}\n",
    "                col_index=vocab.get(word,-1)\n",
    "                if col_index!=-1:\n",
    "                    rows.append(index)\n",
    "                    cols.append(col_index)\n",
    "                    for word,freq in tf.items():\n",
    "                        tfidf= freq * idf[word]\n",
    "                values.append(tfidf)\n",
    "        return normalize(csr_matrix((values,(rows,cols)),shape=(len(corpus),len(vocab))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(corpus):\n",
    "    sent_count=0\n",
    "    main=[] \n",
    "    if isinstance(corpus,(list)):\n",
    "        for sent in corpus:\n",
    "            sen_list=[]\n",
    "            for word in sent.split():\n",
    "                sen_list.append(word)\n",
    "            main.append(sen_list)\n",
    "        unique=set()\n",
    "        tf=[]\n",
    "        for each_sent in main:\n",
    "            n=len(each_sent)\n",
    "            for each_word in each_sent:\n",
    "                unique.add(each_word)\n",
    "                word_freq=dict(Counter(each_word.split()))\n",
    "                for k,v in word_freq.items():\n",
    "                    term_freq=float(v/n) \n",
    "                    tf.append(term_freq)\n",
    "            vocab={j:i for i,j in enumerate(sorted(unique))}\n",
    "                \n",
    "                            \n",
    "                    \n",
    "        store=[]\n",
    "        list_idf=[]\n",
    "        \n",
    "        for each_word in unique:  \n",
    "            count=0 \n",
    "            for i in range(len(main)):\n",
    "                 if each_word in main[i]:\n",
    "                    count+=1    \n",
    "            store.append(count)  \n",
    "            for i in range(0,len(store)):  \n",
    "                idf1=1+(math.log((1+len(main))/(1+store[i])))   \n",
    "            list_idf.append(idf1)\n",
    "        idf=dict(zip(vocab,list_idf))  \n",
    "        top50_idf=dict(sorted(idf.items(),key=itemgetter(1),reverse=True)[:50])\n",
    "        vocab1={word:idf for idf,word in enumerate(top50_idf)}\n",
    "        \n",
    "                       \n",
    "    return  top50_idf,vocab1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
